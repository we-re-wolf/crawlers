# crawlers
Crawlers are used to create a site map. It is a very useful tool which helps in red teaming to find directories as well as files and subdomains.

This program has two parts(for now), the grabber helps capturing all the backlinks which are present in a particular website and the crawlers help in finding all the subdomains and directories and files present in an web application.

Feel free to contribute and please install all the libraries after looking at the code.
